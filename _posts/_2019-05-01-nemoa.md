---
layout: post
title: 'Do we need a paradigm shift in data science?'
author: "Patrick Michl"
license: "CC BY 4.0"
license-url: "https://creativecommons.org/licenses/by/4.0/"
image: "https://www.frootlab.org/images/back/robot.webp"
categories:
  - Corporate
tags:
  - Nemoa
---

**Many experienced data scientists sooner or later recognize a simple
truth about the AI revolution: All that glitters is not gold!**

[![AI Revolution](/images/posts/AI-Revolution.png)](/images/posts/AI-Revolution.png)

It's not by chance, that the Harvard Business Review in 2012 crowned "data
scientist" to be the [sexiest job of the 21st
century](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century).
This choice was not surprising as the arising AI Revolution created a
distinctive gap between supply and demand on the job market. Seven years later,
however, it's time to take stock of progress with respect to the job market's
self-regulation and the new challenges arising therefrom.

### What is the "AI Revolution"?

When talking about the "AI Revolution" it's difficult to narrow down a common
denominator. This is not only because science fiction didn't prepare us for our
first real encounters with AI, but also due to it's multiple appearances. By
taking a closer look, however, most current advances can be traced back to a few
milestones:

The Big-Bang of deep learning was Geoffrey Hinton's daredevil 2006 [Science
Article](https://www.cs.toronto.edu/~hinton/science.pdf). This can be
summarized as follows: (1) Certain undirected models, termed RBMs, can
efficiently be trained to represent data (because they are rapid mixing) and (2)
these RBMs can be stacked to "pre-train" deep directed models. The key is,
that these deep directed models outperform any classical models in predicting
missing values (if trained carefully and by using sufficient data).



In 2012
a group about Yoshua Bengio greatly improved Hinton's approach by welding the
stack of RBMs into a single D
https://arxiv.org/abs/1203.4416

the process by:



. Put simply,
this means that you only need enough data to train a model on prediction.

 empirical observation in 2006: Undirectly graphical models can be learned
efficiently

Bipartite graphical models are rapid mixing. and the resulting [Science
Article](https://www.cs.toronto.edu/~hinton/science.pdf)

Geoffrey
Hinton's daredevil [Science
Article](https://www.cs.toronto.edu/~hinton/science.pdf)



particularly significant aspect, however, that may be considered as the driving
force behind most current progresses is "deep learning".  


From a mathematical point of view, the current development can be
understood on the basis of some key publications

Although it's roots can be traced back


and practical
experiences made so far.



Some years ago - in the early 2010s - when Google's
[TensorFlow®](https://www.tensorflow.org/) still was only an idea and Geoffrey
Hinton's daredevil [Science
Article](https://www.cs.toronto.edu/~hinton/science.pdf) still only received a
bunch of citations, the undisputed technical issues in data science were the
absence of computing power and the absence of a common play ground. Of course,
during the last decade, NVIDIA and Google respectively stepped into
the breach with [CUDA®](https://developer.nvidia.com/cuda-zone) and TensorFlow®.
So the question arises "What are today's foremost technical obstacles in data
science?". In the following I present our personally experienced proposal to
this question and our vision: The *Liquid Coding* framework.












What is this and why is it a problem?
AutoML

The accelerated publication rate, is not only caused by an increased demand, but
also due to the very nature of this area: The more you already know about your
data domain (wise people call it "belief"), the better your estimations for the
observed sample can be. This simple Bayesian wisdom has great implications for
data science: The publications are not only getting more in numbers, but also
in the numbers of partially overlapping domains!

So it should be quite clear, what the issue is. But what are the current tools,
to address it? Corresponding to their individual data domains, the research
papers in data science are distributed among different platforms. A typical
example, that I would like to pick is [arXiv](https://arxiv.org/) (pronounced
*archive*). Since the early days of the web, this 'Grande Dame' essentially
serves as a plain repository for PDF pre-prints. And since data science papers
usually deal with algorithms, these PDFs are quite often endowed with pseudo
code, which basically allows their implementation in any programming language of
choice.

It is quite obvious, that this organically grown [Paper Bottleneck]({% post_url
2019-03-20-three-obstacles-in-data-science %}) has substantial drawbacks: Due to
the limited space, the provided pseudo code often loses valuable details over
the original algorithm. But also if the original algorithm is provided online,
it can be grueling to properly identify it's scope and adapt it to the
underlying prerequisites - only to decide about it's suitability!

### What is Motley?

In a nutshell: The development- and exploration process in data science is
currently heavily impaired by the detour experienced by publications in paper
form. While it became easier and easier to publish,it became harder and harder
to get an overview. This is why we want to provide a better solution!

[Motley](/motley.html) is a smart algorithm repository server, that enforces
unified data interfaces for different algorithm categories. This allows Motley
not only to automatically evaluate and compare the hosted algorithms with
respect to given metrics but thereupon also to determine, which algorithm of a
given category and data domain is the currently best fitting ([CBF](/tags#CBF))
algorithm with respect to some metric. An example for such a metric would be the
average prediction accuracy within a fixed set of gold standard samples of the
respective domain of application (e.g. latin handwriting samples, spoken word
samples, TCGA gene expression data, etc.).

According to our [convictions](/corporate/2019/03/19/welcome-at-frootlab.html),
Motley is [free & open
source](https://www.gnu.org/philosophy/floss-and-foss.en.html), based on the
[Python](https://www.python.org/) programming language and actively developed as
part of our [Liquid Coding](https://github.com/orgs/frootlab/projects)
framework. For more information please visit
[GitHub](https://github.com/frootlab/motley).
